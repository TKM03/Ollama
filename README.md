# 🚀 Local Generative AI with Ollama in Jupyter Notebook

## 📌 Project Overview
This project allows you to run a **free, local Generative AI model** using **Ollama** and interact with it through **Jupyter Notebook**. You can generate text using models like **Mistral 7B, LLaMA, or Gemma** entirely offline, without any API keys or internet dependency.

## 🔧 Technologies Used
- **Python** (Jupyter Notebook)
- **Ollama** (Local LLM Engine)
- **Mistral 7B / LLaMA / Gemma** (LLM Models)
- **Requests** (Handling API calls)

## 📂 Project Structure
```
local-ai-ollama/
│── Ollama.ipynb                 # Jupyter Notebook for AI text generation
│── README.md                    # Project documentation
```

## 🛠 Installation Guide
### **1️⃣ Install Ollama** (Manually)
If you haven’t installed Ollama yet, download it from:
- **Windows/macOS/Linux**: [Ollama Official Website](https://ollama.com/download)

Alternatively, install it via the terminal (**Mac/Linux only**):
```sh
curl -fsSL https://ollama.ai/install.sh | sh
```

### **2️⃣ Start Ollama** (Before Running Jupyter Notebook)
Before running the notebook, **start Ollama in a separate terminal**:
```sh
ollama serve
```

### **3️⃣ Install Required Python Packages**
Inside **Anaconda PowerShell** or any terminal, run:
```sh
pip install requests
```



## 🏆 Features
✅ **Completely free** (No API keys required)  
✅ **Runs 100% offline** (Privacy-focused AI)  
✅ **Supports multiple models** (Mistral, LLaMA, Gemma)  
✅ **Jupyter Notebook-based** (Easy to modify and test)  



## 📬 Contact
For any questions or suggestions, reach out:
- **Email**: your-email@example.com
