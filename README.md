# ğŸš€ Local Generative AI with Ollama in Jupyter Notebook

## ğŸ“Œ Project Overview
This project allows you to run a **free, local Generative AI model** using **Ollama** and interact with it through **Jupyter Notebook**. You can generate text using models like **Mistral 7B, LLaMA, or Gemma** entirely offline, without any API keys or internet dependency.

## ğŸ”§ Technologies Used
- **Python** (Jupyter Notebook)
- **Ollama** (Local LLM Engine)
- **Mistral 7B / LLaMA / Gemma** (LLM Models)
- **Requests** (Handling API calls)

## ğŸ“‚ Project Structure
```
local-ai-ollama/
â”‚â”€â”€ Ollama.ipynb                 # Jupyter Notebook for AI text generation
â”‚â”€â”€ README.md                    # Project documentation
```

## ğŸ›  Installation Guide
### **1ï¸âƒ£ Install Ollama** (Manually)
If you havenâ€™t installed Ollama yet, download it from:
- **Windows/macOS/Linux**: [Ollama Official Website](https://ollama.com/download)

Alternatively, install it via the terminal (**Mac/Linux only**):
```sh
curl -fsSL https://ollama.ai/install.sh | sh
```

### **2ï¸âƒ£ Start Ollama** (Before Running Jupyter Notebook)
Before running the notebook, **start Ollama in a separate terminal**:
```sh
ollama serve
```

### **3ï¸âƒ£ Install Required Python Packages**
Inside **Anaconda PowerShell** or any terminal, run:
```sh
pip install requests
```



## ğŸ† Features
âœ… **Completely free** (No API keys required)  
âœ… **Runs 100% offline** (Privacy-focused AI)  
âœ… **Supports multiple models** (Mistral, LLaMA, Gemma)  
âœ… **Jupyter Notebook-based** (Easy to modify and test)  



## ğŸ“¬ Contact
For any questions or suggestions, reach out:
- **Email**: your-email@example.com
